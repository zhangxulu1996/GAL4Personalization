{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from clip_eval import CLIPEvaluator\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers import CLIPTextModel\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def init_generative_model(args):\n",
    "    \"\"\"\n",
    "    Initialize the model\n",
    "    params:\n",
    "        args: argparse.Namespace\n",
    "    \"\"\"\n",
    "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id).to(args.device)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def check_identifier_token_dir(checkpoint, concept_str):\n",
    "    if os.path.exists(os.path.join(checkpoint, concept_str)):\n",
    "        dir = os.path.join(checkpoint, concept_str)\n",
    "    elif os.path.exists(checkpoint.format(concept_str)):\n",
    "        dir = checkpoint.format(concept_str)\n",
    "    else:\n",
    "        dir = checkpoint\n",
    "    return dir\n",
    "\n",
    "\n",
    "def load_trained_weights(args, pipe, checkpoint, concepts_str, c_identifier):\n",
    "    \"\"\"\n",
    "    Load additional weights for the model\n",
    "    params:\n",
    "        model_name: str\n",
    "        pipe: StableDiffusionPipeline\n",
    "        checkpoint: str, where the additional unique identifier weights are saved\n",
    "        concepts_str: str, the concepts used to generate images\n",
    "    \"\"\"\n",
    "    print(f\"loading unique identifier weights for {concepts_str} ...\")\n",
    "    if args.model_name == 'sd-v1-5':\n",
    "        return pipe\n",
    "    elif args.model_name == 'dreambooth':\n",
    "        dir = check_identifier_token_dir(checkpoint, concepts_str)\n",
    "        unet = UNet2DConditionModel.from_pretrained(os.path.join(dir, 'unet')).to(pipe.device)\n",
    "        pipe.unet = unet\n",
    "        if os.path.exists(os.path.join(dir, 'text_encoder')):\n",
    "            text_encoder = CLIPTextModel.from_pretrained(os.path.join(dir, 'text_encoder')).to(pipe.device)\n",
    "            pipe.text_encoer = text_encoder\n",
    "        for cls in concepts_str.split(','):\n",
    "            if os.path.exists(os.path.join(dir, f\"{c_identifier[cls]}.bin\")):\n",
    "                print(f\"loading unique identifier weights from {c_identifier[cls]}.bin ...\")\n",
    "                pipe.load_textual_inversion(os.path.join(dir, f\"{c_identifier[cls]}.bin\"))\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model name {args.model_name}')\n",
    "    return pipe\n",
    "\n",
    "def check_mk_file_dir(file_name):\n",
    "    check_mkdir(file_name[:file_name.rindex(\"/\")])\n",
    "    \n",
    "def check_mkdir(dir_name):\n",
    "    \"\"\"\n",
    "    check if the folder exists, if not exists, the func will create the new named folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "def save_img(im, prompts, save_dir, name):\n",
    "    \"\"\"\n",
    "    Save images\n",
    "    params:\n",
    "        im: numpy array\n",
    "        prompts: list\n",
    "        save_dir: str\n",
    "        name: str\n",
    "    \"\"\"\n",
    "    save_path = os.path.join(save_dir, f'{prompts}/{name}.jpg')\n",
    "    check_mk_file_dir(save_path)\n",
    "    if isinstance(im, np.ndarray):\n",
    "        Image.fromarray(im).save(save_path)\n",
    "    elif isinstance(im, Image.Image):\n",
    "        im.save(save_path)\n",
    "    else:\n",
    "        raise TypeError(f'Unknown type {type(im)}') \n",
    "\n",
    "def get_reference_images(concepts_str, src_img_dir):\n",
    "    src_imgs = {}\n",
    "    for concept in concepts_str.split(','):\n",
    "        src_imgs[concept] = []\n",
    "        for img_path in os.listdir(os.path.join(src_img_dir, concept)):\n",
    "            src_imgs[concept].append(Image.open(os.path.join(src_img_dir, concept, img_path)))\n",
    "    return src_imgs\n",
    "\n",
    "\n",
    "def edit_original_prompt(prompt, c_identifier, keys, mode='replace'):\n",
    "    \"\"\"\n",
    "    Edit the original prompt to the prompt with identifiers\n",
    "    params:\n",
    "        prompt: str\n",
    "        c_identifier: dict, concepts and identifiers, e.g. {'cat': '<cute-cat>'}\n",
    "        keys: concept list, e.g. ['cat', 'dog']\n",
    "        mode: str, 'replace' or 'insert'\n",
    "    returns:\n",
    "        prompt: str\n",
    "    \"\"\"\n",
    "    if mode != 'none':\n",
    "        for key in keys:\n",
    "            if '_' in key:\n",
    "                replace_str = key.replace('_', ' ')\n",
    "            else:\n",
    "                replace_str = key\n",
    "            if replace_str in prompt:\n",
    "                if mode == 'replace':\n",
    "                    prompt = prompt.replace(replace_str, c_identifier[key])\n",
    "                elif mode == 'insert':\n",
    "                    prompt = prompt.replace(replace_str, f'{c_identifier[key]} {replace_str}')\n",
    "            else:\n",
    "                raise ValueError(f'{replace_str} not in prompt {prompt}')\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "def generate_images(args, c_p, c_identifier):\n",
    "    \"\"\"\n",
    "    Generate images from the optimization-based model\n",
    "    params:\n",
    "        args: argparse.Namespace\n",
    "        c_p: dict, concepts and prompts, e.g. {'cat': ['a photo of a cat']}\n",
    "        c_identifier: dict, concepts and identifiers, e.g. {'cat': '<cute-cat>'}\n",
    "    \"\"\"\n",
    "    for concepts_str, prompts in c_p.items():\n",
    "        pipe = init_generative_model(args)\n",
    "        pipe = load_trained_weights(args, pipe, args.checkpoint, concepts_str, c_identifier)\n",
    "\n",
    "        for prompt in prompts:\n",
    "            edited_prompt = edit_original_prompt(prompt, c_identifier, concepts_str.split(','), mode=args.edit_mode)\n",
    "            print(\"Generating images for prompt: \", edited_prompt)\n",
    "            generator = torch.manual_seed(8888)\n",
    "            for idx in range(args.num_per_prompt):\n",
    "                im = pipe(edited_prompt, num_inference_steps=50, guidance_scale=7.5, generator=generator).images[0]\n",
    "                save_img(im, prompt, os.path.join(args.img_save_dir, concepts_str), idx)\n",
    "    print(\"all images saved in \", args.img_save_dir)\n",
    "\n",
    "\n",
    "def check_dir(img_save_dir, concepts_str, prompt):\n",
    "    \"\"\"\n",
    "    check dir if exists for alignment function\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(img_save_dir, concepts_str, prompt)):\n",
    "        dir = os.path.join(img_save_dir, concepts_str, prompt)\n",
    "    else:\n",
    "        raise ValueError(f'no such dir: {os.path.join(img_save_dir, concepts_str, prompt)}')\n",
    "    return dir\n",
    "\n",
    "\n",
    "def eval_alignment(concepts_list, evaluator, c_p, img_save_dir, src_img_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the alignment between the generated images and the source images/texts\n",
    "    params:\n",
    "        concepts_list: list\n",
    "        evaluator: CLIPEvaluator\n",
    "        c_p: dict, concepts and prompts\n",
    "        img_save_dir: str, where the generated images are saved\n",
    "        src_img_dir: str, where the source images are saved\n",
    "    returns:\n",
    "        img_img_sim_mean: dict, the average similarity between the generated images and the source images\n",
    "        text_img_sim_mean: dict, the average similarity between the generated images and the source texts\n",
    "    \"\"\"\n",
    "    img_img_sim = {}   # {c_str: {c1:[s1, s2, s3], c2:[s1, s2, s3]}}\n",
    "    text_img_sim = {}  # {c_str: [s1, s2, s3]}\n",
    "    overfitting = {}\n",
    "    \n",
    "    for concepts_str, prompts in c_p.items():\n",
    "        print(f'evaluating {concepts_str}...')\n",
    "        img_img_sim[concepts_str] = {} \n",
    "        text_img_sim[concepts_str] = []\n",
    "        overfitting[concepts_str] = []\n",
    "\n",
    "        src_imgs = get_reference_images(concepts_str, src_img_dir) # {c1: [I1, I2...], c2: [I1, I2...]}\n",
    "        src_img_features = {}  # {c1: [f1, f2, f3...], c2: [f1, f2, f3...}\n",
    "        for concept, concept_src_imgs in src_imgs.items():\n",
    "            src_img_features[concept] = [evaluator.get_image_features(src_img) for src_img in concept_src_imgs]\n",
    "            img_img_sim[concepts_str][concept] = []\n",
    "\n",
    "        src_cap_features = {} # {c1: [f1, f2, f3...], c2: [f1, f2, f3...}\n",
    "        src_captions = {concept:concepts_list[concept][\"caption\"] for concept in concepts_str.split(',')}\n",
    "        for concept, concept_src_captions in src_captions.items():\n",
    "            src_cap_features[concept] = [evaluator.get_text_features(src_caption) for src_caption in concept_src_captions]\n",
    "\n",
    "\n",
    "        for prompt in prompts:\n",
    "            text_feature = evaluator.get_text_features(prompt)\n",
    "            dir = check_dir(img_save_dir, concepts_str, prompt)\n",
    "            for img_path in os.listdir(dir):\n",
    "                img = Image.open(os.path.join(dir, img_path))\n",
    "                # text alignment\n",
    "                text_img_sim[concepts_str].append(2.5*evaluator.txt_to_img_similarity(img, text_features=text_feature).cpu().numpy())\n",
    "                # image alignment\n",
    "                for concept, concept_img_features in src_img_features.items():\n",
    "                    img_img_sim[concepts_str][concept].extend([evaluator.img_to_img_similarity(img, src_img_features=src_img_feature).cpu().numpy() for src_img_feature in concept_img_features])\n",
    "                \n",
    "                # overfitting\n",
    "                overfitting_flag = False\n",
    "                for concept, concept_cap_features in src_cap_features.items():\n",
    "                    cap_img_sim = [2.5*evaluator.txt_to_img_similarity(img, text_features=src_cap_feature).cpu().numpy() for src_cap_feature in concept_cap_features]\n",
    "                    # compare text_img_sim[concepts_str][-1] with cap_img_sim\n",
    "                    if text_img_sim[concepts_str][-1] < np.max(cap_img_sim):\n",
    "                        print(f'overfitting: {prompt} {img_path} < {src_captions[concept][np.argmax(cap_img_sim)]} {text_img_sim[concepts_str][-1]} {np.max(cap_img_sim)}')\n",
    "                        overfitting_flag = True\n",
    "\n",
    "                if overfitting_flag == False:\n",
    "                    overfitting[concepts_str].append(0)\n",
    "                else:\n",
    "                    overfitting[concepts_str].append(1)\n",
    "    \n",
    "    img_img_sim_mean = {}\n",
    "    text_img_sim_mean = {}\n",
    "    overfitting_mean = {}\n",
    "    for concepts_str, sim_list in img_img_sim.items():\n",
    "        img_img_sim_mean[concepts_str] = {}\n",
    "        for concept, sims in sim_list.items():\n",
    "            img_img_sim_mean[concepts_str][concept] = np.mean(sims)\n",
    "        text_img_sim_mean[concepts_str] = np.mean(text_img_sim[concepts_str])\n",
    "        overfitting_mean[concepts_str] = np.mean(overfitting[concepts_str])\n",
    "        print(f'{concepts_str}: image alignment -- {img_img_sim_mean[concepts_str]} | text alignment -- {text_img_sim_mean[concepts_str]}')\n",
    "    return {'img': img_img_sim_mean, 'text': text_img_sim_mean}\n",
    "\n",
    "\n",
    "def get_placeholders(concepts_list):\n",
    "    \"\"\"\n",
    "    Get the placeholders for the concepts\n",
    "    params:\n",
    "        concepts_list: list, the concepts list\n",
    "    \"\"\"\n",
    "    cls_identifier = {}\n",
    "    for concept in concepts_list:\n",
    "        cls_identifier[concept['class_prompt']] = concept['placeholder']\n",
    "    return cls_identifier\n",
    "\n",
    "def load_prompts_from_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        prompts = f.read().split('\\n')\n",
    "    return prompts\n",
    "\n",
    "def get_concept_prompts(concepts_list):\n",
    "    \"\"\"\n",
    "    Get the concept prompts\n",
    "    params:\n",
    "        concepts_list: list, the concepts list\n",
    "    returns:\n",
    "        c_p: dict, {concept_str: [prompts]}, e.g. {'cat': ['a cat in front of a desk']}\n",
    "    \"\"\"\n",
    "    p_c = {}\n",
    "    for concept in concepts_list:\n",
    "        for prompt in load_prompts_from_file(concept[f'test_prompts']):\n",
    "            if prompt not in p_c:\n",
    "                p_c[prompt] = [concept['class_prompt']]\n",
    "            elif concept['class_prompt'] not in p_c[prompt]:\n",
    "                p_c[prompt].append(concept['class_prompt'])\n",
    "    c_p = {}\n",
    "    for prompt, concepts in p_c.items():\n",
    "        for concept in concepts:\n",
    "            if concept not in c_p:\n",
    "                c_p[concept] = [prompt]\n",
    "            elif prompt not in c_p[concept]:\n",
    "                c_p[concept].append(prompt)\n",
    "    return c_p\n",
    "\n",
    "def set_seed(seed = 8888):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def generate(args):\n",
    "    set_seed(seed=8888)\n",
    "    # prepare the prompts\n",
    "    concepts_list = json.load(open(args.concepts_list_path, \"r\"))\n",
    "    c_p = get_concept_prompts(concepts_list)   # dict: {concept_str: [prompts]}\n",
    "    c_identifier = get_placeholders(concepts_list)\n",
    "    # generate images\n",
    "    generate_images(args, c_p, c_identifier)\n",
    "\n",
    "\n",
    "def evaluate(args):\n",
    "    \"\"\"\n",
    "    Evaluate the generated images\n",
    "    params:\n",
    "        args: argparse.Namespace\n",
    "        eval_clip_score: bool, whether to evaluate the CLIP score (including image alignment and text-image alignment)\n",
    "        eval_coco_coi: bool, whether to evaluate the coco CoI score\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    concepts_list = json.load(open(args.concepts_list_path, \"r\"))\n",
    "    c_p = get_concept_prompts(concepts_list)   # dict: {concept_str: [prompts]}\n",
    "    \n",
    "    # evaluate the image alignment and text-image alignment\n",
    "    evaluator = CLIPEvaluator(args.device)\n",
    "    clip_score = eval_alignment(concepts_list, evaluator, c_p, args.img_save_dir, args.src_img_dir)\n",
    "    results.update(clip_score)\n",
    "\n",
    "    save_results(args, results)\n",
    "\n",
    "def run_and_test(args, **kwargs):\n",
    "    generate(args)\n",
    "    evaluate(args, **kwargs)\n",
    "    \n",
    "def save_results(args, scores):\n",
    "    \"\"\" \n",
    "    Save the results to a csv file\n",
    "    params:\n",
    "        args: argparse.Namespace\n",
    "        scores: dict, {score_name: {concept_str: score}}\n",
    "    \"\"\"\n",
    "    results = args.get_dict()\n",
    "    if os.path.exists(args.results_path):\n",
    "        file = pd.read_csv(args.results_path)\n",
    "    else:\n",
    "        file = pd.DataFrame(columns=list(results.keys()))\n",
    "    \n",
    "    for score_name, score_dict in scores.items():\n",
    "        for concept_str, score in score_dict.items():\n",
    "            if isinstance(score, dict):\n",
    "                for concept, s in score.items():\n",
    "                    results[f'{concept_str}_{concept}_{score_name}'] = s\n",
    "            else:\n",
    "                results[f'{concept_str}_{score_name}'] = score\n",
    "    file = pd.concat([file, pd.DataFrame(results, index=[0])], ignore_index=True)\n",
    "    file.to_csv(args.results_path, index=False)\n",
    "    print(\"results saved in \", args.results_path)\n",
    "\n",
    "class Dict2Class(object):\n",
    "    def __init__(self, mydict):\n",
    "        self.dict = mydict\n",
    "        for key in mydict.keys():\n",
    "            setattr(self, key, mydict[key])\n",
    "\n",
    "    def get_dict(self):\n",
    "        return self.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"model_name\": \"dreambooth\",  \n",
    "    \"device\": \"cuda:0\",\n",
    "    \"edit_mode\": \"insert\",\n",
    "    \"num_per_prompt\": 2,  # number of generated images per prompt\n",
    "    \"concepts_list_path\": \"./data/concepts_list_object.json\",\n",
    "    \"checkpoint\": \"./snapshot/gal_obj/{}\",   \n",
    "    \"src_img_dir\": \"./data/object/\",\n",
    "    \"results_path\": \"results.csv\",\n",
    "}\n",
    "args[\"img_save_dir\"] = \"./samples/{}\".format(args['model_name'])\n",
    "args = Dict2Class(args)\n",
    "run_and_test(args, eval_clip_score=True, eval_coco_coi=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
